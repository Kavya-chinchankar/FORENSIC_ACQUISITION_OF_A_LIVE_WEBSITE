# Website Forensic Analysis using FAW

## ğŸ“Œ Project Overview

This project demonstrates the **forensic acquisition and analysis of a website** using the **FAW (Forensic Acquisition of Website)** tool. The objective is to collect, preserve, and analyze website-related digital evidence in a **forensically sound and legally acceptable manner**.

Website forensics plays a critical role in investigating **phishing attacks, website defacement, data theft, and cyber fraud**, where preserving evidence integrity is essential for investigations and legal proceedings.

This project was completed as part of an **academic internship in Cyber Security**.

---

## ğŸ¯ Objectives

* Perform **forensic acquisition of a live website** without altering original data
* Preserve website content with **cryptographic hash verification**
* Capture:

  * Web pages and linked resources
  * Metadata and server responses
  * Network traffic artifacts
* Generate **forensic reports** suitable for investigation and documentation
* Analyze captured traffic using **Wireshark**

---

## ğŸ› ï¸ Tools & Technologies Used

* **FAW (Forensic Acquisition of Website â€“ Lite)**
* **Wireshark**
* Web Browsers: Chromium / Chrome / Firefox
* Hashing Algorithm: **SHA-256**
* Protocols: HTTP / HTTPS / TCP / DNS

---

## ğŸ’» System Requirements

### Hardware

* Processor: Intel i3 or higher
* RAM: Minimum 4 GB (8 GB recommended)
* Storage: 20 GB free disk space
* Architecture: 64-bit

### Software

* OS: Windows 10 / Windows 11 / Linux
* FAW Tool
* Wireshark
* Stable Internet Connection

---

## ğŸ” Methodology

1. Create a new forensic case in FAW
2. Configure acquisition parameters (depth, linked resources)
3. Initiate website acquisition using Chromium Lite
4. Capture:

   * HTML pages
   * Images, scripts, stylesheets
   * Metadata and logs
5. Generate cryptographic hash values
6. Save evidence in structured case folders
7. Analyze network traffic using Wireshark

---

## ğŸ“‚ Evidence Collected

* Website source files (HTML, CSS, JS)
* Linked resources (images, scripts)
* Metadata and HTTP headers
* Cryptographic hash values
* Network traffic capture files (PCAP)
* Forensic acquisition logs
* Generated forensic reports (HTML/PDF)

---

## ğŸ“Š Analysis Performed

Using **Wireshark**, the following were analyzed:

* DNS queries (domain resolution)
* TCP connections between client and server
* TLS-encrypted traffic
* Website communication patterns

---

## âœ… Results

* Website data successfully acquired in a **forensically sound manner**
* Evidence integrity verified using hash values
* Network traffic captured and analyzed accurately
* All artifacts preserved with proper documentation
* Generated reports suitable for **digital investigations and academic evaluation**

---

## ğŸ§  Key Learnings

* Importance of **forensic integrity and chain-of-custody**
* Difference between normal website downloading and forensic acquisition
* Practical understanding of web protocols in investigations
* Hands-on experience with **real forensic tools**
* Ethical and legal considerations in cyber investigations

---

## âš ï¸ Ethical Notice

> This project was performed **strictly for educational and academic purposes**.
> All forensic activities were conducted in controlled environments, following ethical and legal cybersecurity guidelines.

---

## ğŸ‘©â€ğŸ’» Author

**Kavya Krishna Chinchankar**
M.Tech â€“ Cyber Security
The Oxford College of Engineering, Bangalore

---

## ğŸ“œ License

This project is intended for **educational use only**.
Unauthorized or malicious use of forensic techniques is strictly discouraged.

---

